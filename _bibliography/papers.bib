---
---

@article{RestoreAnyone,
  title={RestoreAnyone: Identity-consistent Face Video Restoration with Reference Guidance},
  author={Chen, Ziyan
  and Xie, Liangbin
  and Liu, Yihao
  and Li, Zheyuan
  and Yu, Fanghua
  and Dong, Chao
  },
  journal={Manuscript},
  year={2025},
  pdf={https://drive.google.com/file/d/1qZtbZGg0gmyEk9mSrCai0PxyjgtTxrtO/view?usp=drive_link},
  preview={RestoreAnyone.gif},
  selected={true},
}

@InProceedings{10.1007/978-3-031-73202-7_25,
  author="Lin*, Xinqi
    and He*, Jingwen
    and Chen, Ziyan
    and Lyu, Zhaoyang
    and Dai, Bo
    and Yu, Fanghua
    and Qiao, Yu
    and Ouyang, Wanli
    and Dong, Chao",
  title="DiffBIR: Toward Blind Image Restoration with Generative Diffusion Prior",
  booktitle="Proceedings of the European Conference on Computer Vision (ECCV)",
  year="2024",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="430--448",
  abstract="We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance quality and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR's superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. The code is available at https://github.com/XPixelGroup/DiffBIR.",
  isbn="978-3-031-73202-7",
  selected={true},
  preview={DiffBIR.png},
  html={https://0x3f3f3f3fun.github.io/projects/diffbir/},
  pdf={https://arxiv.org/pdf/2308.15070},
  google_scholar_id={UeHWp8X0CEIC},
}

@InProceedings{Han_2023_ICCV,
    author    = {Li, Xiaohui and Liu, Yihao and Cao, Shuo and Chen, Ziyan and Zhuang, Shaobin and Chen, Xiangyu and He, Yinan and Wang, Yi and Qiao, Yu},
    title     = {DiffVSR: Enhancing Real-world Video Super-resolution with Diffusion Models for Advanced Visual Quality and Temporal Consistency},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2025},
    html      = {https://xh9998.github.io/DiffVSR-project/},
    video     = {https://www.youtube.com/embed/ezRM2xF3fDw},
    pdf       = {https://arxiv.org/pdf/2501.10110},
    preview   = {DiffVSR.gif},
    google_scholar_id={eQOLeE2rZwMC}
}

@inproceedings{chen2024towards,
  title={Towards real-world video face restoration: A new benchmark},
  author={Chen*, Ziyan and He*, Jingwen and Lin, Xinqi and Qiao, Yu and Dong, Chao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={5929--5939},
  year={2024},
  pdf={https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Chen_Towards_Real-world_Video_Face_Restoration_A_New_Benchmark_CVPRW_2024_paper.pdf},
  abstract={Blind image face restoration (BFR) has significantly progressed over the last several years while the real-world video face restoration (VFR) which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved remains unsolved. Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images which are limited in their coverage of real-world video frames. In this work we introduce a new benchmark dataset named FOS with a taxonomy of Full Occluded and Side faces from both video frames and images with not only synthetic and real-world degradations covered but also rich facial data contents such as various expressions poses and ethnicity involved. Given the established datasets we benchmark both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches revealing their potential and limitations in VFR tasks. For performance assessment we not only employ the commonly used general image quality assessment (IQA) metrics but also explore face IQA metrics by leveraging an elaborately designed user study. With extensive experimental results and detailed analysis provided we gain insights from the successes and failures of current methods. In addition these studies also pose challenges in current BFR approaches and shed light on future VFR research.},
  selected={true},
  html={https://ziyannchen.github.io/projects/VFRxBenchmark/},
  preview={VFRxBench.gif},
  google_scholar_id={YsMSGLbcyi4C}
}

@InProceedings{Huang_2023_CVPR,
    author    = {Hao*, Huang and Chen*, Ziyan and Chen*, Huanran and Wang, Yongtao and Zhang, Kevin},
    title     = {T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {20514-20523},
    selected  = {true},
    preview   = {T-SEA.gif},
    video     = {https://www.youtube.com/watch?v=1rkwIvv1KJ0},
    pdf       = {https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf},
    abstract  = {Compared to query-based black-box attacks, transfer-based black-box attacks do not require any information of the attacked models, which ensures their secrecy. However, most existing transfer-based approaches rely on ensembling multiple models to boost the attack transferability, which is time-and resource-intensive, not to mention the difficulty of obtaining diverse models on the same task. To address this limitation, in this work, we focus on the single-model transfer-based black-box attack on object detection, utilizing only one model to achieve a high-transferability adversarial attack on multiple black-box detectors. Specifically, we first make observations on the patch optimization process of the existing method and propose an enhanced attack framework by slightly adjusting its training strategies. Then, we analogize patch optimization with regular model optimization, proposing a series of self-ensemble approaches on the input data, the attacked model, and the adversarial patch to efficiently make use of the limited information and prevent the patch from overfitting. The experimental results show that the proposed framework can be applied with multiple classical base attack methods (eg, PGD and MIM) to greatly improve the black-box transferability of the well-optimized patch on multiple mainstream detectors, meanwhile boosting white-box performance.},
    google_scholar_id={qjMakFHDy7sC}
}